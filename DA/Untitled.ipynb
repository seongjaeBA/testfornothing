{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchase_counts = df.groupby(['group', 'is_purchase']).user_id.count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics stats¶\n",
    "This section collects various statistical tests and tools. Some can be used independently of any models, some are intended as extension to the models and model results.\n",
    "\n",
    "API Warning: The functions and objects in this category are spread out in various modules and might still be moved around. We expect that in future the statistical tests will return class instances with more informative reporting instead of only the raw numbers.\n",
    "\n",
    "Residual Diagnostics and Specification Tests¶\n",
    "durbin_watson(resids[, axis])\n",
    "\n",
    "Calculates the Durbin-Watson statistic.\n",
    "\n",
    "jarque_bera(resids[, axis])\n",
    "\n",
    "The Jarque-Bera test of normality.\n",
    "\n",
    "omni_normtest(resids[, axis])\n",
    "\n",
    "Omnibus test for normality\n",
    "\n",
    "medcouple(y[, axis])\n",
    "\n",
    "Calculate the medcouple robust measure of skew.\n",
    "\n",
    "robust_skewness(y[, axis])\n",
    "\n",
    "Calculates the four skewness measures in Kim & White\n",
    "\n",
    "robust_kurtosis(y[, axis, ab, dg, excess])\n",
    "\n",
    "Calculates the four kurtosis measures in Kim & White\n",
    "\n",
    "expected_robust_kurtosis([ab, dg])\n",
    "\n",
    "Calculates the expected value of the robust kurtosis measures in Kim and White assuming the data are normally distributed.\n",
    "\n",
    "acorr_breusch_godfrey(res[, nlags, store])\n",
    "\n",
    "Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n",
    "\n",
    "acorr_ljungbox(x[, lags, boxpierce, …])\n",
    "\n",
    "Ljung-Box test of autocorrelation in residuals.\n",
    "\n",
    "acorr_lm(resid[, nlags, autolag, store, …])\n",
    "\n",
    "Lagrange Multiplier tests for autocorrelation.\n",
    "\n",
    "breaks_cusumolsresid(resid[, ddof])\n",
    "\n",
    "Cusum test for parameter stability based on ols residuals.\n",
    "\n",
    "breaks_hansen(olsresults)\n",
    "\n",
    "Test for model stability, breaks in parameters for ols, Hansen 1992\n",
    "\n",
    "recursive_olsresiduals(res[, skip, lamda, …])\n",
    "\n",
    "Calculate recursive ols with residuals and Cusum test statistic\n",
    "\n",
    "compare_cox(results_x, results_z[, store])\n",
    "\n",
    "Compute the Cox test for non-nested models\n",
    "\n",
    "compare_encompassing(results_x, results_z[, …])\n",
    "\n",
    "Davidson-MacKinnon encompassing test for comparing non-nested models\n",
    "\n",
    "compare_j(results_x, results_z[, store])\n",
    "\n",
    "Compute the J-test for non-nested models\n",
    "\n",
    "het_arch(resid[, nlags, autolag, store, ddof])\n",
    "\n",
    "Engle’s Test for Autoregressive Conditional Heteroscedasticity (ARCH).\n",
    "\n",
    "het_breuschpagan(resid, exog_het[, robust])\n",
    "\n",
    "Breusch-Pagan Lagrange Multiplier test for heteroscedasticity\n",
    "\n",
    "het_goldfeldquandt(y, x[, idx, split, drop, …])\n",
    "\n",
    "Goldfeld-Quandt homoskedasticity test.\n",
    "\n",
    "het_white(resid, exog)\n",
    "\n",
    "White’s Lagrange Multiplier Test for Heteroscedasticity.\n",
    "\n",
    "spec_white(resid, exog)\n",
    "\n",
    "White’s Two-Moment Specification Test\n",
    "\n",
    "linear_harvey_collier(res[, order_by, skip])\n",
    "\n",
    "Harvey Collier test for linearity\n",
    "\n",
    "linear_lm(resid, exog[, func])\n",
    "\n",
    "Lagrange multiplier test for linearity against functional alternative\n",
    "\n",
    "linear_rainbow(res[, frac, order_by, …])\n",
    "\n",
    "Rainbow test for linearity\n",
    "\n",
    "linear_reset(res[, power, test_type, use_f, …])\n",
    "\n",
    "Ramsey’s RESET test for neglected nonlinearity\n",
    "\n",
    "Outliers and influence measures¶\n",
    "OLSInfluence(results)\n",
    "\n",
    "class to calculate outlier and influence measures for OLS result\n",
    "\n",
    "GLMInfluence(results[, resid, endog, exog, …])\n",
    "\n",
    "Influence and outlier measures (experimental)\n",
    "\n",
    "MLEInfluence(results[, resid, endog, exog, …])\n",
    "\n",
    "Local Influence and outlier measures (experimental)\n",
    "\n",
    "variance_inflation_factor(exog, exog_idx)\n",
    "\n",
    "variance inflation factor, VIF, for one exogenous variable\n",
    "\n",
    "See also the notes on notes on regression diagnostics\n",
    "\n",
    "Sandwich Robust Covariances¶\n",
    "The following functions calculate covariance matrices and standard errors for the parameter estimates that are robust to heteroscedasticity and autocorrelation in the errors. Similar to the methods that are available for the LinearModelResults, these methods are designed for use with OLS.\n",
    "\n",
    "sandwich_covariance.cov_hac(results[, …])\n",
    "\n",
    "heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\n",
    "\n",
    "sandwich_covariance.cov_nw_panel(results, …)\n",
    "\n",
    "Panel HAC robust covariance matrix\n",
    "\n",
    "sandwich_covariance.cov_nw_groupsum(results, …)\n",
    "\n",
    "Driscoll and Kraay Panel robust covariance matrix\n",
    "\n",
    "sandwich_covariance.cov_cluster(results, group)\n",
    "\n",
    "cluster robust covariance matrix\n",
    "\n",
    "sandwich_covariance.cov_cluster_2groups(…)\n",
    "\n",
    "cluster robust covariance matrix for two groups/clusters\n",
    "\n",
    "sandwich_covariance.cov_white_simple(results)\n",
    "\n",
    "heteroscedasticity robust covariance matrix (White)\n",
    "\n",
    "The following are standalone versions of the heteroscedasticity robust standard errors attached to LinearModelResults\n",
    "\n",
    "sandwich_covariance.cov_hc0(results)\n",
    "\n",
    "See statsmodels.RegressionResults\n",
    "\n",
    "sandwich_covariance.cov_hc1(results)\n",
    "\n",
    "See statsmodels.RegressionResults\n",
    "\n",
    "sandwich_covariance.cov_hc2(results)\n",
    "\n",
    "See statsmodels.RegressionResults\n",
    "\n",
    "sandwich_covariance.cov_hc3(results)\n",
    "\n",
    "See statsmodels.RegressionResults\n",
    "\n",
    "sandwich_covariance.se_cov(cov)\n",
    "\n",
    "get standard deviation from covariance matrix\n",
    "\n",
    "Goodness of Fit Tests and Measures¶\n",
    "some tests for goodness of fit for univariate distributions\n",
    "\n",
    "powerdiscrepancy(observed, expected[, …])\n",
    "\n",
    "Calculates power discrepancy, a class of goodness-of-fit tests as a measure of discrepancy between observed and expected data.\n",
    "\n",
    "gof_chisquare_discrete(distfn, arg, rvs, …)\n",
    "\n",
    "perform chisquare test for random sample of a discrete distribution\n",
    "\n",
    "gof_binning_discrete(rvs, distfn, arg[, nsupp])\n",
    "\n",
    "get bins for chisquare type gof tests for a discrete distribution\n",
    "\n",
    "chisquare_effectsize(probs0, probs1[, …])\n",
    "\n",
    "effect size for a chisquare goodness-of-fit test\n",
    "\n",
    "anderson_statistic(x[, dist, fit, params, axis])\n",
    "\n",
    "Calculate the Anderson-Darling a2 statistic.\n",
    "\n",
    "normal_ad(x[, axis])\n",
    "\n",
    "Anderson-Darling test for normal distribution unknown mean and variance.\n",
    "\n",
    "kstest_exponential(x, *[, dist, pvalmethod])\n",
    "\n",
    "Test assumed normal or exponential distribution using Lilliefors’ test.\n",
    "\n",
    "kstest_fit(x[, dist, pvalmethod])\n",
    "\n",
    "Test assumed normal or exponential distribution using Lilliefors’ test.\n",
    "\n",
    "kstest_normal(x[, dist, pvalmethod])\n",
    "\n",
    "Test assumed normal or exponential distribution using Lilliefors’ test.\n",
    "\n",
    "lilliefors(x[, dist, pvalmethod])\n",
    "\n",
    "Test assumed normal or exponential distribution using Lilliefors’ test.\n",
    "\n",
    "Non-Parametric Tests¶\n",
    "mcnemar(x[, y, exact, correction])\n",
    "\n",
    "McNemar test\n",
    "\n",
    "symmetry_bowker(table)\n",
    "\n",
    "Test for symmetry of a (k, k) square contingency table\n",
    "\n",
    "median_test_ksample(x, groups)\n",
    "\n",
    "chisquare test for equality of median/location\n",
    "\n",
    "runstest_1samp(x[, cutoff, correction])\n",
    "\n",
    "use runs test on binary discretized data above/below cutoff\n",
    "\n",
    "runstest_2samp(x[, y, groups, correction])\n",
    "\n",
    "Wald-Wolfowitz runstest for two samples\n",
    "\n",
    "cochrans_q(x)\n",
    "\n",
    "Cochran’s Q test for identical effect of k treatments\n",
    "\n",
    "Runs(x)\n",
    "\n",
    "class for runs in a binary sequence\n",
    "\n",
    "sign_test(samp[, mu0])\n",
    "\n",
    "Signs test\n",
    "\n",
    "rank_compare_2indep(x1, x2[, use_t])\n",
    "\n",
    "Statistics and tests for the probability that x1 has larger values than x2.\n",
    "\n",
    "rank_compare_2ordinal(count1, count2[, …])\n",
    "\n",
    "stochastically larger probability for 2 independend ordinal samples\n",
    "\n",
    "cohensd2problarger(d)\n",
    "\n",
    "convert Cohen’s d effect size to stochastically-larger-probability\n",
    "\n",
    "prob_larger_continuous(distr1, distr2)\n",
    "\n",
    "probability indicating that distr1 is stochastically larger than distr2\n",
    "\n",
    "rankdata_2samp(x1, x2)\n",
    "\n",
    "Compute midranks for two samples\n",
    "\n",
    "Descriptive Statistics¶\n",
    "describe(data[, stats, numeric, …])\n",
    "\n",
    "Extended descriptive statistics for data\n",
    "\n",
    "Description(data, pandas.core.series.Series, …)\n",
    "\n",
    "Extended descriptive statistics for data\n",
    "\n",
    "Interrater Reliability and Agreement¶\n",
    "The main function that statsmodels has currently available for interrater agreement measures and tests is Cohen’s Kappa. Fleiss’ Kappa is currently only implemented as a measures but without associated results statistics.\n",
    "\n",
    "cohens_kappa(table[, weights, …])\n",
    "\n",
    "Compute Cohen’s kappa with variance and equal-zero test\n",
    "\n",
    "fleiss_kappa(table[, method])\n",
    "\n",
    "Fleiss’ and Randolph’s kappa multi-rater agreement measure\n",
    "\n",
    "to_table(data[, bins])\n",
    "\n",
    "convert raw data with shape (subject, rater) to (rater1, rater2)\n",
    "\n",
    "aggregate_raters(data[, n_cat])\n",
    "\n",
    "convert raw data with shape (subject, rater) to (subject, cat_counts)\n",
    "\n",
    "Multiple Tests and Multiple Comparison Procedures¶\n",
    "multipletests is a function for p-value correction, which also includes p-value correction based on fdr in fdrcorrection. tukeyhsd performs simultaneous testing for the comparison of (independent) means. These three functions are verified. GroupsStats and MultiComparison are convenience classes to multiple comparisons similar to one way ANOVA, but still in development\n",
    "\n",
    "multipletests(pvals[, alpha, method, …])\n",
    "\n",
    "Test results and p-value correction for multiple tests\n",
    "\n",
    "fdrcorrection(pvals[, alpha, method, is_sorted])\n",
    "\n",
    "pvalue correction for false discovery rate\n",
    "\n",
    "GroupsStats(x[, useranks, uni, intlab])\n",
    "\n",
    "statistics by groups (another version)\n",
    "\n",
    "MultiComparison(data, groups[, group_order])\n",
    "\n",
    "Tests for multiple comparisons\n",
    "\n",
    "TukeyHSDResults(mc_object, results_table, q_crit)\n",
    "\n",
    "Results from Tukey HSD test, with additional plot methods\n",
    "\n",
    "pairwise_tukeyhsd(endog, groups[, alpha])\n",
    "\n",
    "Calculate all pairwise comparisons with TukeyHSD confidence intervals\n",
    "\n",
    "local_fdr(zscores[, null_proportion, …])\n",
    "\n",
    "Calculate local FDR values for a list of Z-scores.\n",
    "\n",
    "fdrcorrection_twostage(pvals[, alpha, …])\n",
    "\n",
    "(iterated) two stage linear step-up procedure with estimation of number of true hypotheses\n",
    "\n",
    "NullDistribution(zscores[, null_lb, …])\n",
    "\n",
    "Estimate a Gaussian distribution for the null Z-scores.\n",
    "\n",
    "RegressionFDR(endog, exog, regeffects[, method])\n",
    "\n",
    "Control FDR in a regression procedure.\n",
    "\n",
    "CorrelationEffects()\n",
    "\n",
    "Marginal correlation effect sizes for FDR control.\n",
    "\n",
    "OLSEffects()\n",
    "\n",
    "OLS regression for knockoff analysis.\n",
    "\n",
    "ForwardEffects(pursuit)\n",
    "\n",
    "Forward selection effect sizes for FDR control.\n",
    "\n",
    "OLSEffects()\n",
    "\n",
    "OLS regression for knockoff analysis.\n",
    "\n",
    "RegModelEffects(model_cls[, regularized, …])\n",
    "\n",
    "Use any regression model for Regression FDR analysis.\n",
    "\n",
    "The following functions are not (yet) public\n",
    "\n",
    "varcorrection_pairs_unbalanced(nobs_all[, …])\n",
    "\n",
    "correction factor for variance with unequal sample sizes for all pairs\n",
    "\n",
    "varcorrection_pairs_unequal(var_all, …)\n",
    "\n",
    "return joint variance from samples with unequal variances and unequal sample sizes for all pairs\n",
    "\n",
    "varcorrection_unbalanced(nobs_all[, srange])\n",
    "\n",
    "correction factor for variance with unequal sample sizes\n",
    "\n",
    "varcorrection_unequal(var_all, nobs_all, df_all)\n",
    "\n",
    "return joint variance from samples with unequal variances and unequal sample sizes\n",
    "\n",
    "StepDown(vals, nobs_all, var_all[, df])\n",
    "\n",
    "a class for step down methods\n",
    "\n",
    "catstack(args)\n",
    "\n",
    "ccols\n",
    "\n",
    "compare_ordered(vals, alpha)\n",
    "\n",
    "simple ordered sequential comparison of means\n",
    "\n",
    "distance_st_range(mean_all, nobs_all, var_all)\n",
    "\n",
    "pairwise distance matrix, outsourced from tukeyhsd\n",
    "\n",
    "ecdf(x)\n",
    "\n",
    "no frills empirical cdf used in fdrcorrection\n",
    "\n",
    "get_tukeyQcrit(k, df[, alpha])\n",
    "\n",
    "return critical values for Tukey’s HSD (Q)\n",
    "\n",
    "homogeneous_subsets(vals, dcrit)\n",
    "\n",
    "recursively check all pairs of vals for minimum distance\n",
    "\n",
    "maxzero(x)\n",
    "\n",
    "find all up zero crossings and return the index of the highest\n",
    "\n",
    "maxzerodown(x)\n",
    "\n",
    "find all up zero crossings and return the index of the highest\n",
    "\n",
    "mcfdr([nrepl, nobs, ntests, ntrue, mu, …])\n",
    "\n",
    "MonteCarlo to test fdrcorrection\n",
    "\n",
    "qcrit\n",
    "\n",
    "str(object=’’) -> str str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
    "\n",
    "randmvn(rho[, size, standardize])\n",
    "\n",
    "create random draws from equi-correlated multivariate normal distribution\n",
    "\n",
    "rankdata(x)\n",
    "\n",
    "rankdata, equivalent to scipy.stats.rankdata\n",
    "\n",
    "rejectionline(n[, alpha])\n",
    "\n",
    "reference line for rejection in multiple tests\n",
    "\n",
    "set_partition(ssli)\n",
    "\n",
    "extract a partition from a list of tuples\n",
    "\n",
    "set_remove_subs(ssli)\n",
    "\n",
    "remove sets that are subsets of another set from a list of tuples\n",
    "\n",
    "tiecorrect(xranks)\n",
    "\n",
    "should be equivalent of scipy.stats.tiecorrect\n",
    "\n",
    "Basic Statistics and t-Tests with frequency weights¶\n",
    "Besides basic statistics, like mean, variance, covariance and correlation for data with case weights, the classes here provide one and two sample tests for means. The t-tests have more options than those in scipy.stats, but are more restrictive in the shape of the arrays. Confidence intervals for means are provided based on the same assumptions as the t-tests.\n",
    "\n",
    "Additionally, tests for equivalence of means are available for one sample and for two, either paired or independent, samples. These tests are based on TOST, two one-sided tests, which have as null hypothesis that the means are not “close” to each other.\n",
    "\n",
    "DescrStatsW(data[, weights, ddof])\n",
    "\n",
    "Descriptive statistics and tests with weights for case weights\n",
    "\n",
    "CompareMeans(d1, d2)\n",
    "\n",
    "class for two sample comparison\n",
    "\n",
    "ttest_ind(x1, x2[, alternative, usevar, …])\n",
    "\n",
    "ttest independent sample\n",
    "\n",
    "ttost_ind(x1, x2, low, upp[, usevar, …])\n",
    "\n",
    "test of (non-)equivalence for two independent samples\n",
    "\n",
    "ttost_paired(x1, x2, low, upp[, transform, …])\n",
    "\n",
    "test of (non-)equivalence for two dependent, paired sample\n",
    "\n",
    "ztest(x1[, x2, value, alternative, usevar, ddof])\n",
    "\n",
    "test for mean based on normal distribution, one or two samples\n",
    "\n",
    "ztost(x1, low, upp[, x2, usevar, ddof])\n",
    "\n",
    "Equivalence test based on normal distribution\n",
    "\n",
    "zconfint(x1[, x2, value, alpha, …])\n",
    "\n",
    "confidence interval based on normal distribution z-test\n",
    "\n",
    "weightstats also contains tests and confidence intervals based on summary data\n",
    "\n",
    "_tconfint_generic(mean, std_mean, dof, …)\n",
    "\n",
    "generic t-confint based on summary statistic\n",
    "\n",
    "_tstat_generic(value1, value2, std_diff, …)\n",
    "\n",
    "generic ttest based on summary statistic\n",
    "\n",
    "_zconfint_generic(mean, std_mean, alpha, …)\n",
    "\n",
    "generic normal-confint based on summary statistic\n",
    "\n",
    "_zstat_generic(value1, value2, std_diff, …)\n",
    "\n",
    "generic (normal) z-test based on summary statistic\n",
    "\n",
    "_zstat_generic2(value, std, alternative)\n",
    "\n",
    "generic (normal) z-test based on summary statistic\n",
    "\n",
    "Power and Sample Size Calculations¶\n",
    "The power module currently implements power and sample size calculations for the t-tests, normal based test, F-tests and Chisquare goodness of fit test. The implementation is class based, but the module also provides three shortcut functions, tt_solve_power, tt_ind_solve_power and zt_ind_solve_power to solve for any one of the parameters of the power equations.\n",
    "\n",
    "TTestIndPower(**kwds)\n",
    "\n",
    "Statistical Power calculations for t-test for two independent sample\n",
    "\n",
    "TTestPower(**kwds)\n",
    "\n",
    "Statistical Power calculations for one sample or paired sample t-test\n",
    "\n",
    "GofChisquarePower(**kwds)\n",
    "\n",
    "Statistical Power calculations for one sample chisquare test\n",
    "\n",
    "NormalIndPower([ddof])\n",
    "\n",
    "Statistical Power calculations for z-test for two independent samples.\n",
    "\n",
    "FTestAnovaPower(**kwds)\n",
    "\n",
    "Statistical Power calculations F-test for one factor balanced ANOVA\n",
    "\n",
    "FTestPower(**kwds)\n",
    "\n",
    "Statistical Power calculations for generic F-test\n",
    "\n",
    "normal_power_het(diff, nobs, alpha[, …])\n",
    "\n",
    "Calculate power of a normal distributed test statistic\n",
    "\n",
    "normal_sample_size_one_tail(diff, power, alpha)\n",
    "\n",
    "explicit sample size computation if only one tail is relevant\n",
    "\n",
    "tt_solve_power([effect_size, nobs, alpha, …])\n",
    "\n",
    "solve for any one parameter of the power of a one sample t-test\n",
    "\n",
    "tt_ind_solve_power([effect_size, nobs1, …])\n",
    "\n",
    "solve for any one parameter of the power of a two sample t-test\n",
    "\n",
    "zt_ind_solve_power([effect_size, nobs1, …])\n",
    "\n",
    "solve for any one parameter of the power of a two sample z-test\n",
    "\n",
    "Proportion¶\n",
    "Also available are hypothesis test, confidence intervals and effect size for proportions that can be used with NormalIndPower.\n",
    "\n",
    "proportion_confint(count, nobs[, alpha, method])\n",
    "\n",
    "confidence interval for a binomial proportion\n",
    "\n",
    "proportion_effectsize(prop1, prop2[, method])\n",
    "\n",
    "Effect size for a test comparing two proportions\n",
    "\n",
    "binom_test(count, nobs[, prop, alternative])\n",
    "\n",
    "Perform a test that the probability of success is p.\n",
    "\n",
    "binom_test_reject_interval(value, nobs[, …])\n",
    "\n",
    "rejection region for binomial test for one sample proportion\n",
    "\n",
    "binom_tost(count, nobs, low, upp)\n",
    "\n",
    "exact TOST test for one proportion using binomial distribution\n",
    "\n",
    "binom_tost_reject_interval(low, upp, nobs[, …])\n",
    "\n",
    "rejection region for binomial TOST\n",
    "\n",
    "multinomial_proportions_confint(counts[, …])\n",
    "\n",
    "Confidence intervals for multinomial proportions.\n",
    "\n",
    "proportions_ztest(count, nobs[, value, …])\n",
    "\n",
    "Test for proportions based on normal (z) test\n",
    "\n",
    "proportions_ztost(count, nobs, low, upp[, …])\n",
    "\n",
    "Equivalence test based on normal distribution\n",
    "\n",
    "proportions_chisquare(count, nobs[, value])\n",
    "\n",
    "test for proportions based on chisquare test\n",
    "\n",
    "proportions_chisquare_allpairs(count, nobs)\n",
    "\n",
    "chisquare test of proportions for all pairs of k samples\n",
    "\n",
    "proportions_chisquare_pairscontrol(count, nobs)\n",
    "\n",
    "chisquare test of proportions for pairs of k samples compared to control\n",
    "\n",
    "proportion_effectsize(prop1, prop2[, method])\n",
    "\n",
    "Effect size for a test comparing two proportions\n",
    "\n",
    "power_binom_tost(low, upp, nobs[, p_alt, alpha])\n",
    "\n",
    "power_ztost_prop(low, upp, nobs, p_alt[, …])\n",
    "\n",
    "Power of proportions equivalence test based on normal distribution\n",
    "\n",
    "samplesize_confint_proportion(proportion, …)\n",
    "\n",
    "find sample size to get desired confidence interval length\n",
    "\n",
    "Statistics for two independent samples Status: experimental, API might change, added in 0.12\n",
    "\n",
    "test_proportions_2indep(count1, nobs1, …)\n",
    "\n",
    "Hypothesis test for comparing two independent proportions\n",
    "\n",
    "confint_proportions_2indep(count1, nobs1, …)\n",
    "\n",
    "Confidence intervals for comparing two independent proportions\n",
    "\n",
    "power_proportions_2indep(diff, prop2, nobs1)\n",
    "\n",
    "power for ztest that two independent proportions are equal\n",
    "\n",
    "tost_proportions_2indep(count1, nobs1, …)\n",
    "\n",
    "Equivalence test based on two one-sided test_proportions_2indep\n",
    "\n",
    "samplesize_proportions_2indep_onetail(diff, …)\n",
    "\n",
    "required sample size assuming normal distribution based on one tail\n",
    "\n",
    "score_test_proportions_2indep(count1, nobs1, …)\n",
    "\n",
    "score_test for two independent proportions\n",
    "\n",
    "_score_confint_inversion(count1, nobs1, …)\n",
    "\n",
    "Compute score confidence interval by inverting score test\n",
    "\n",
    "Rates¶\n",
    "Statistical functions for rates. This currently includes hypothesis tests for two independent samples.\n",
    "\n",
    "Status: experimental, API might change, added in 0.12\n",
    "\n",
    "test_poisson_2indep(count1, exposure1, …)\n",
    "\n",
    "test for ratio of two sample Poisson intensities\n",
    "\n",
    "etest_poisson_2indep(count1, exposure1, …)\n",
    "\n",
    "E-test for ratio of two sample Poisson rates\n",
    "\n",
    "tost_poisson_2indep(count1, exposure1, …)\n",
    "\n",
    "Equivalence test based on two one-sided test_proportions_2indep\n",
    "\n",
    "Multivariate¶\n",
    "Statistical functions for multivariate samples.\n",
    "\n",
    "This includes hypothesis test and confidence intervals for mean of sample of multivariate observations and hypothesis tests for the structure of a covariance matrix.\n",
    "\n",
    "Status: experimental, API might change, added in 0.12\n",
    "\n",
    "test_mvmean(data[, mean_null, return_results])\n",
    "\n",
    "Hotellings test for multivariate mean in one sample\n",
    "\n",
    "confint_mvmean(data[, lin_transf, alpha, simult])\n",
    "\n",
    "Confidence interval for linear transformation of a multivariate mean\n",
    "\n",
    "confint_mvmean_fromstats(mean, cov, nobs[, …])\n",
    "\n",
    "Confidence interval for linear transformation of a multivariate mean\n",
    "\n",
    "test_mvmean_2indep(data1, data2)\n",
    "\n",
    "Hotellings test for multivariate mean in two independent samples\n",
    "\n",
    "test_cov(cov, nobs, cov_null)\n",
    "\n",
    "One sample hypothesis test for covariance equal to null covariance\n",
    "\n",
    "test_cov_blockdiagonal(cov, nobs, block_len)\n",
    "\n",
    "One sample hypothesis test that covariance is block diagonal.\n",
    "\n",
    "test_cov_diagonal(cov, nobs)\n",
    "\n",
    "One sample hypothesis test that covariance matrix is diagonal matrix.\n",
    "\n",
    "test_cov_oneway(cov_list, nobs_list)\n",
    "\n",
    "Multiple sample hypothesis test that covariance matrices are equal.\n",
    "\n",
    "test_cov_spherical(cov, nobs)\n",
    "\n",
    "One sample hypothesis test that covariance matrix is spherical\n",
    "\n",
    "Oneway Anova¶\n",
    "Hypothesis test, confidence intervals and effect size for oneway analysis of k samples.\n",
    "\n",
    "Status: experimental, API might change, added in 0.12\n",
    "\n",
    "anova_oneway(data[, groups, use_var, …])\n",
    "\n",
    "Oneway Anova\n",
    "\n",
    "anova_generic(means, variances, nobs[, …])\n",
    "\n",
    "Oneway Anova based on summary statistics\n",
    "\n",
    "equivalence_oneway(data, equiv_margin[, …])\n",
    "\n",
    "equivalence test for oneway anova (Wellek’s Anova)\n",
    "\n",
    "equivalence_oneway_generic(f_stat, n_groups, …)\n",
    "\n",
    "Equivalence test for oneway anova (Wellek and extensions)\n",
    "\n",
    "power_equivalence_oneway(f2_alt, …[, …])\n",
    "\n",
    "Power of oneway equivalence test\n",
    "\n",
    "_power_equivalence_oneway_emp(f_stat, …[, …])\n",
    "\n",
    "Empirical power of oneway equivalence test\n",
    "\n",
    "test_scale_oneway(data[, method, center, …])\n",
    "\n",
    "Oneway Anova test for equal scale, variance or dispersion\n",
    "\n",
    "equivalence_scale_oneway(data, equiv_margin)\n",
    "\n",
    "Oneway Anova test for equivalence of scale, variance or dispersion\n",
    "\n",
    "confint_effectsize_oneway(f_stat, df[, …])\n",
    "\n",
    "Confidence interval for effect size in oneway anova for F distribution\n",
    "\n",
    "confint_noncentrality(f_stat, df[, alpha, …])\n",
    "\n",
    "Confidence interval for noncentrality parameter in F-test\n",
    "\n",
    "convert_effectsize_fsqu([f2, eta2])\n",
    "\n",
    "Convert squared effect sizes in f family\n",
    "\n",
    "effectsize_oneway(means, vars_, nobs[, …])\n",
    "\n",
    "Effect size corresponding to Cohen’s f = nc / nobs for oneway anova\n",
    "\n",
    "f2_to_wellek(f2, n_groups)\n",
    "\n",
    "Convert Cohen’s f-squared to Wellek’s effect size (sqrt)\n",
    "\n",
    "fstat_to_wellek(f_stat, n_groups, nobs_mean)\n",
    "\n",
    "Convert F statistic to wellek’s effect size eps squared\n",
    "\n",
    "wellek_to_f2(eps, n_groups)\n",
    "\n",
    "Convert Wellek’s effect size (sqrt) to Cohen’s f-squared\n",
    "\n",
    "_fstat2effectsize(f_stat, df)\n",
    "\n",
    "Compute anova effect size from F-statistic\n",
    "\n",
    "scale_transform(data[, center, transform, …])\n",
    "\n",
    "Transform data for variance comparison for Levene type tests\n",
    "\n",
    "simulate_power_equivalence_oneway(means, …)\n",
    "\n",
    "Simulate Power for oneway equivalence test (Wellek’s Anova)\n",
    "\n",
    "Robust, Trimmed Statistics¶\n",
    "Statistics for samples that are trimmed at a fixed fraction. This includes class TrimmedMean for one sample statistics. It is used in stats.oneway for trimmed “Yuen” Anova.\n",
    "\n",
    "Status: experimental, API might change, added in 0.12\n",
    "\n",
    "TrimmedMean(data, fraction[, is_sorted, axis])\n",
    "\n",
    "class for trimmed and winsorized one sample statistics\n",
    "\n",
    "scale_transform(data[, center, transform, …])\n",
    "\n",
    "Transform data for variance comparison for Levene type tests\n",
    "\n",
    "trim_mean(a, proportiontocut[, axis])\n",
    "\n",
    "Return mean of array after trimming observations from both lower and upper tails.\n",
    "\n",
    "trimboth(a, proportiontocut[, axis])\n",
    "\n",
    "Slices off a proportion of items from both ends of an array.\n",
    "\n",
    "Moment Helpers¶\n",
    "When there are missing values, then it is possible that a correlation or covariance matrix is not positive semi-definite. The following three functions can be used to find a correlation or covariance matrix that is positive definite and close to the original matrix.\n",
    "\n",
    "corr_clipped(corr[, threshold])\n",
    "\n",
    "Find a near correlation matrix that is positive semi-definite\n",
    "\n",
    "corr_nearest(corr[, threshold, n_fact])\n",
    "\n",
    "Find the nearest correlation matrix that is positive semi-definite.\n",
    "\n",
    "corr_nearest_factor(corr, rank[, ctol, …])\n",
    "\n",
    "Find the nearest correlation matrix with factor structure to a given square matrix.\n",
    "\n",
    "corr_thresholded(data[, minabs, max_elt])\n",
    "\n",
    "Construct a sparse matrix containing the thresholded row-wise correlation matrix from a data array.\n",
    "\n",
    "cov_nearest(cov[, method, threshold, …])\n",
    "\n",
    "Find the nearest covariance matrix that is positive (semi-) definite\n",
    "\n",
    "cov_nearest_factor_homog(cov, rank)\n",
    "\n",
    "Approximate an arbitrary square matrix with a factor-structured matrix of the form k*I + XX’.\n",
    "\n",
    "FactoredPSDMatrix(diag, root)\n",
    "\n",
    "Representation of a positive semidefinite matrix in factored form.\n",
    "\n",
    "kernel_covariance(exog, loc, groups[, …])\n",
    "\n",
    "Use kernel averaging to estimate a multivariate covariance function.\n",
    "\n",
    "These are utility functions to convert between central and non-central moments, skew, kurtosis and cummulants.\n",
    "\n",
    "cum2mc(kappa)\n",
    "\n",
    "convert non-central moments to cumulants recursive formula produces as many cumulants as moments\n",
    "\n",
    "mc2mnc(mc)\n",
    "\n",
    "convert central to non-central moments, uses recursive formula optionally adjusts first moment to return mean\n",
    "\n",
    "mc2mvsk(args)\n",
    "\n",
    "convert central moments to mean, variance, skew, kurtosis\n",
    "\n",
    "mnc2cum(mnc)\n",
    "\n",
    "convert non-central moments to cumulants recursive formula produces as many cumulants as moments\n",
    "\n",
    "mnc2mc(mnc[, wmean])\n",
    "\n",
    "convert non-central to central moments, uses recursive formula optionally adjusts first moment to return mean\n",
    "\n",
    "mnc2mvsk(args)\n",
    "\n",
    "convert central moments to mean, variance, skew, kurtosis\n",
    "\n",
    "mvsk2mc(args)\n",
    "\n",
    "convert mean, variance, skew, kurtosis to central moments\n",
    "\n",
    "mvsk2mnc(args)\n",
    "\n",
    "convert mean, variance, skew, kurtosis to non-central moments\n",
    "\n",
    "cov2corr(cov[, return_std])\n",
    "\n",
    "convert covariance matrix to correlation matrix\n",
    "\n",
    "corr2cov(corr, std)\n",
    "\n",
    "convert correlation matrix to covariance matrix given standard deviation\n",
    "\n",
    "se_cov(cov)\n",
    "\n",
    "get standard deviation from covariance matrix\n",
    "\n",
    "Mediation Analysis¶\n",
    "Mediation analysis focuses on the relationships among three key variables: an ‘outcome’, a ‘treatment’, and a ‘mediator’. Since mediation analysis is a form of causal inference, there are several assumptions involved that are difficult or impossible to verify. Ideally, mediation analysis is conducted in the context of an experiment such as this one in which the treatment is randomly assigned. It is also common for people to conduct mediation analyses using observational data in which the treatment may be thought of as an ‘exposure’. The assumptions behind mediation analysis are even more difficult to verify in an observational setting.\n",
    "\n",
    "Mediation(outcome_model, mediator_model, …)\n",
    "\n",
    "Conduct a mediation analysis.\n",
    "\n",
    "MediationResults(indirect_effects, …)\n",
    "\n",
    "A class for holding the results of a mediation analysis.\n",
    "\n",
    "Oaxaca-Blinder Decomposition¶\n",
    "The Oaxaca-Blinder, or Blinder-Oaxaca as some call it, decomposition attempts to explain gaps in means of groups. It uses the linear models of two given regression equations to show what is explained by regression coefficients and known data and what is unexplained using the same data. There are two types of Oaxaca-Blinder decompositions, the two-fold and the three-fold, both of which can and are used in Economics Literature to discuss differences in groups. This method helps classify discrimination or unobserved effects. This function attempts to port the functionality of the oaxaca command in STATA to Python.\n",
    "\n",
    "OaxacaBlinder(endog, exog, bifurcate[, …])\n",
    "\n",
    "Class to perform Oaxaca-Blinder Decomposition.\n",
    "\n",
    "OaxacaResults(results, model_type)\n",
    "\n",
    "This class summarizes the fit of the OaxacaBlinder model.\n",
    "\n",
    "Distance Dependence Measures¶\n",
    "Distance dependence measures and the Distance Covariance (dCov) test.\n",
    "\n",
    "distance_covariance_test(x, y[, B, method])\n",
    "\n",
    "The Distance Covariance (dCov) test\n",
    "\n",
    "distance_statistics(x, y[, x_dist, y_dist])\n",
    "\n",
    "Calculate various distance dependence statistics.\n",
    "\n",
    "distance_correlation(x, y)\n",
    "\n",
    "Distance correlation.\n",
    "\n",
    "distance_covariance(x, y)\n",
    "\n",
    "Distance covariance.\n",
    "\n",
    "distance_variance(x)\n",
    "\n",
    "Distance variance.\n",
    "\n",
    "Meta-Analysis¶\n",
    "Functions for basic meta-analysis of a collection of sample statistics.\n",
    "\n",
    "Examples can be found in the notebook\n",
    "\n",
    "Meta-Analysis\n",
    "\n",
    "Status: experimental, API might change, added in 0.12\n",
    "\n",
    "combine_effects(effect, variance[, …])\n",
    "\n",
    "combining effect sizes for effect sizes using meta-analysis\n",
    "\n",
    "effectsize_2proportions(count1, nobs1, …)\n",
    "\n",
    "Effects sizes for two sample binomial proportions\n",
    "\n",
    "effectsize_smd(mean1, sd1, nobs1, mean2, …)\n",
    "\n",
    "effect sizes for mean difference for use in meta-analysis\n",
    "\n",
    "CombineResults(**kwds)\n",
    "\n",
    "Results from combined estimate of means or effect sizes\n",
    "\n",
    "The module also includes internal functions to compute random effects variance.\n",
    "\n",
    "_fit_tau_iter_mm(eff, var_eff[, tau2_start, …])\n",
    "\n",
    "iterated method of moment estimate of between random effect variance\n",
    "\n",
    "_fit_tau_iterative(eff, var_eff[, …])\n",
    "\n",
    "Paule-Mandel iterative estimate of between random effect variance\n",
    "\n",
    "_fit_tau_mm(eff, var_eff, weights)\n",
    "\n",
    "one-step method of moment estimate of between random effect variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
